{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d81bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "412b8867",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = '''I am taking this course, regretting that its too late already and i should've taken it much before but i didn't.\\nIt makes me sad and anxious and feel disappointed in myself, but i need to overcome all those emotions and focus on completing the course and learn some new things.\\nAnd by things i mean things that will get me a job ASAP. lol.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79cd8916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am taking this course, regretting that its too late already and i should've taken it much before but i didn't.\n",
      "It makes me sad and anxious and feel disappointed in myself, but i need to overcome all those emotions and focus on completing the course and learn some new things.\n",
      "And by things i mean things that will get me a job ASAP. lol.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6bc0b8",
   "metadata": {},
   "source": [
    "#1 Tokenizing paragraphs into sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1bc8eb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: I am taking this course, regretting that its too late already and i should've taken it much before but i didn't.\n",
      "1: It makes me sad and anxious and feel disappointed in myself, but i need to overcome all those emotions and focus on completing the course and learn some new things.\n",
      "2: And by things i mean things that will get me a job ASAP.\n",
      "3: lol.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "documents=sent_tokenize(corpus)\n",
    "for i,token in enumerate(documents):\n",
    "    print(f\"{i}: {token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c72a87",
   "metadata": {},
   "source": [
    "#2 Tokenize input into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba93cf88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "am\n",
      "taking\n",
      "this\n",
      "course\n",
      ",\n",
      "regretting\n",
      "that\n",
      "its\n",
      "too\n",
      "late\n",
      "already\n",
      "and\n",
      "i\n",
      "should\n",
      "'ve\n",
      "taken\n",
      "it\n",
      "much\n",
      "before\n",
      "but\n",
      "i\n",
      "did\n",
      "n't\n",
      ".\n",
      "It\n",
      "makes\n",
      "me\n",
      "sad\n",
      "and\n",
      "anxious\n",
      "and\n",
      "feel\n",
      "disappointed\n",
      "in\n",
      "myself\n",
      ",\n",
      "but\n",
      "i\n",
      "need\n",
      "to\n",
      "overcome\n",
      "all\n",
      "those\n",
      "emotions\n",
      "and\n",
      "focus\n",
      "on\n",
      "completing\n",
      "the\n",
      "course\n",
      "and\n",
      "learn\n",
      "some\n",
      "new\n",
      "things\n",
      ".\n",
      "And\n",
      "by\n",
      "things\n",
      "i\n",
      "mean\n",
      "things\n",
      "that\n",
      "will\n",
      "get\n",
      "me\n",
      "a\n",
      "job\n",
      "ASAP\n",
      ".\n",
      "lol\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "words=[]\n",
    "for document in documents:\n",
    "    word=word_tokenize(document)\n",
    "    words.append(word)\n",
    "for word in words:\n",
    "    for w in word:\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f16937",
   "metadata": {},
   "source": [
    "#3 sentences to words, but even punctuations are tokenized into individual units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec69afe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "am\n",
      "taking\n",
      "this\n",
      "course\n",
      ",\n",
      "regretting\n",
      "that\n",
      "its\n",
      "too\n",
      "late\n",
      "already\n",
      "and\n",
      "i\n",
      "should\n",
      "'\n",
      "ve\n",
      "taken\n",
      "it\n",
      "much\n",
      "before\n",
      "but\n",
      "i\n",
      "didn\n",
      "'\n",
      "t\n",
      ".\n",
      "It\n",
      "makes\n",
      "me\n",
      "sad\n",
      "and\n",
      "anxious\n",
      "and\n",
      "feel\n",
      "disappointed\n",
      "in\n",
      "myself\n",
      ",\n",
      "but\n",
      "i\n",
      "need\n",
      "to\n",
      "overcome\n",
      "all\n",
      "those\n",
      "emotions\n",
      "and\n",
      "focus\n",
      "on\n",
      "completing\n",
      "the\n",
      "course\n",
      "and\n",
      "learn\n",
      "some\n",
      "new\n",
      "things\n",
      ".\n",
      "And\n",
      "by\n",
      "things\n",
      "i\n",
      "mean\n",
      "things\n",
      "that\n",
      "will\n",
      "get\n",
      "me\n",
      "a\n",
      "job\n",
      "ASAP\n",
      ".\n",
      "lol\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "words=[]\n",
    "for document in documents:\n",
    "    word=wordpunct_tokenize(document)\n",
    "    words.append(word)\n",
    "for word in words:\n",
    "    for w in word:\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613a4b40",
   "metadata": {},
   "source": [
    "TreebankWordTokenizer <br>\n",
    "A rule-based tokenizer from NLTK that breaks a sentence into word-level tokens using the Penn Treebank conventions. Unlike simple methods like .split(), it handles punctuation, contractions (like \"isn't\" → \"is\", \"n't\"), possessives (\"John's\" → \"John\", \"'s\"), and hyphenated words more accurately.\n",
    "\n",
    "This tokenizer produces cleaner and more linguistically precise tokens, which is especially useful for natural language processing tasks like part-of-speech tagging, parsing, and syntactic analysis.\n",
    "\n",
    "It works best when applied to individual sentences rather than full paragraphs, and it's ideal when you need tokens that align closely with formal linguistic standards. \n",
    "\n",
    "So punctuation marks are treated with semantic meaning, like can't will be separated such that the token represent \"can\" and \"not\" ['ca','n't']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "87ebe074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'taking', 'this', 'course', ',', 'regretting', 'that', 'its', 'too', 'late', 'already', 'and', 'i', 'should', \"'ve\", 'taken', 'it', 'much', 'before', 'but', 'i', \"didn't.\", 'It', 'makes', 'me', 'sad', 'and', 'anxious', 'and', 'feel', 'disappointed', 'in', 'myself', ',', 'but', 'i', 'need', 'to', 'overcome', 'all', 'those', 'emotions', 'and', 'focus', 'on', 'completing', 'the', 'course', 'and', 'learn', 'some', 'new', 'things.', 'And', 'by', 'things', 'i', 'mean', 'things', 'that', 'will', 'get', 'me', 'a', 'job', 'ASAP.', 'lol', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "words=[]\n",
    "tokenizer=TreebankWordTokenizer()\n",
    "tokens=tokenizer.tokenize(corpus)\n",
    "print(tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
